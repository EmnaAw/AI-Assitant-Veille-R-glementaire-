{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c841a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, glob, re, shutil, time\n",
    "from google.colab import userdata\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from huggingface_hub import InferenceClient\n",
    "# NEW CORRECT IMPORT FOR DOCUMENT\n",
    "from langchain_core.documents import Document\n",
    "import chromadb # Import chromadb to access Settings\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "DB_PATH = os.path.abspath(\"./preventis_final_db\") # Convert to absolute path\n",
    "\n",
    "\n",
    "# --- 2. THE RAG PIPELINE (Loading -> Chunking -> Embedding -> Storing) ---\n",
    "if os.path.exists(DB_PATH):\n",
    "    shutil.rmtree(DB_PATH)\n",
    "    time.sleep(1)\n",
    "os.makedirs(DB_PATH, exist_ok=True) # Ensure the directory is recreated and writable\n",
    "\n",
    "\n",
    "def setup_rag_ultra():\n",
    "    pdf_files = glob.glob(\"/content/sample_data/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No PDFs found in /content/sample_data/ !\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    all_docs = []\n",
    "    for pdf in pdf_files:\n",
    "        print(f\"üìñ Loading: {os.path.basename(pdf)}...\")\n",
    "        loader = PyPDFLoader(pdf)\n",
    "        pages = loader.load()\n",
    "        for p in pages:\n",
    "            p.metadata[\"source\"] = os.path.basename(pdf)\n",
    "            all_docs.append(p)\n",
    "\n",
    "\n",
    "    # 1. CHUNKING: We cut the text into 1200-char pieces so Article 294 stays whole.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\nArticle\", \"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(all_docs)\n",
    "    print(f\"‚úÖ Chunking complete: {len(chunks)} blocks created.\")\n",
    "\n",
    "\n",
    "    # 2. EMBEDDING & STORING: MiniLM turns text into math; Chroma stores it.\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "\n",
    "    # Explicitly create a persistent ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "\n",
    "    return Chroma.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        # Removed persist_directory here to avoid conflict when client is explicitly provided\n",
    "        client=chroma_client # Pass the explicit ChromaDB client\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 3. HYBRID SEARCH LOGIC ---\n",
    "def get_context(query, vector_db):\n",
    "    # Keyword Boost (Finds specific article numbers)\n",
    "    nums = re.findall(r'\\d+(?:-\\d+)*', query)\n",
    "    keyword_docs = []\n",
    "    if nums:\n",
    "        data = vector_db.get()\n",
    "        for i, text in enumerate(data['documents']):\n",
    "            if nums[0] in text:\n",
    "                keyword_docs.append(Document(page_content=text, metadata=data['metadatas'][i]))\n",
    "\n",
    "\n",
    "    # Semantic Search\n",
    "    semantic_docs = vector_db.similarity_search(query, k=4)\n",
    "\n",
    "\n",
    "    # Merge\n",
    "    combined = {d.page_content: d for d in (keyword_docs + semantic_docs)}.values()\n",
    "    return list(combined)\n",
    "\n",
    "\n",
    "# --- 4. GENERATION ---\n",
    "def ask_ai(query, vector_db):\n",
    "    docs = get_context(query, vector_db)\n",
    "    context_text = \"\\n\".join([f\"[{d.metadata['source']}]: {d.page_content}\" for d in docs])\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"Role: Expert juridique. R√©pondez UNIQUEMENT en utilisant le contexte. Citez la source comme [Fichier.pdf]. Langue: Fran√ßais.\n",
    "\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "\n",
    "    # Mistral\n",
    "    m_res = requests.post(\"https://api.mistral.ai/v1/chat/completions\",\n",
    "                         headers={\"Authorization\": f\"Bearer {MISTRAL_API_KEY}\"},\n",
    "                         json={\"model\": \"mistral-small-latest\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.1}).json()\n",
    "\n",
    "\n",
    "    # Llama 3.1\n",
    "    client = InferenceClient(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=HF_TOKEN)\n",
    "    l_res = client.chat_completion(messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=800, temperature=0.1)\n",
    "\n",
    "\n",
    "    return m_res['choices'][0]['message']['content'], l_res.choices[0].message.content\n",
    "\n",
    "\n",
    "# --- 5. INTERFACE ---\n",
    "v_db = setup_rag_ultra()\n",
    "if v_db:\n",
    "    while True:\n",
    "        user_q = input(\"\\nüëâ Ask your legal question (or 'exit'): \")\n",
    "        if user_q.lower() == 'exit': break\n",
    "        ans_m, ans_l = ask_ai(user_q, v_db)\n",
    "        print(\"\\n\" + \"=\"*50 + f\"\\nüîµ MISTRAL:\\n{ans_m}\\n\" + \"-\"*25 + f\"\\n\\U0001f99f LLAMA 3.1:\\n{ans_l}\\n\" + \"=\"*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
